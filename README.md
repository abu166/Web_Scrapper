# Web Scraper Project

This project scrapes tablet product data from a test website and stores it in a PostgreSQL database. It uses **Colly** with **ChromeDP** for scraping, **GoQuery** for HTML parsing, and **PostgreSQL** as the storage backend.

## ğŸ§° Features

* Scrapes tablet product data (title, price, description, rating, image URL)
* Stores scraped data into a PostgreSQL database
* Uses Docker for containerization of both application and database
* Implements clean architecture (domain, infrastructure, use case layers)

## ğŸ“¦ Prerequisites

Ensure you have the following installed:
* Docker
* Docker Compose

## ğŸš€ How to Run the Project

### 1. Clone the Repository

```bash
git clone https://github.com/abu166/Web_Scrapper.git
cd web_scrapper
```

### 2. Start the Application Using Docker Compose

```bash
docker-compose up -d --build
```

This will:
* Build and start the Go application container.
* Start a PostgreSQL database container.
* Automatically run the scraper when the app starts.

You should see logs indicating that scraping has completed successfully.

## ğŸ” Checking the Scraped Data

To verify that the tablet data was successfully scraped and stored:

**Option 1: Connect via `psql` inside Docker**

```bash
docker exec -it web_scrapper_db_1 psql -U admin -d web_scrapper
```

Then run:

```sql
SELECT * FROM tablets;
```

You should see all the tablet records inserted by the scraper.

## ğŸ“ Project Structure Overview

```
web_scrapper/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ main.go                 # Entry point of the application
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ model.go               # Tablet struct definition
â”‚   â””â”€â”€ repository.go          # Repository interface
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â””â”€â”€ colly_scraper.go   # ChromeDP + Colly implementation
â”‚   â””â”€â”€ storage/
â”‚       â””â”€â”€ postgres_storage.go # PostgreSQL persistence logic
â”œâ”€â”€ usecase/
â”‚   â”œâ”€â”€ interface.go           # Scraper interface
â”‚   â””â”€â”€ scraper_usecase.go     # Business logic layer
â”œâ”€â”€ migrations/
â”‚   â””â”€â”€ ...                    # Database migration files
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .env                       # Environment variables
â””â”€â”€ README.md                  # General Information
```

## ğŸ› ï¸ Configuration

All configuration is handled through the `.env` file:

```env
PGUSER=admin
PGPASSWORD=admin
PGDATABASE=web_scrapper
PGHOST=db
PGPORT=5432
```

Note: `PGHOST` must match the service name defined in `docker-compose.yml`.

## ğŸ§ª Test Site

The scraper targets this URL:
```
https://webscraper.io/test-sites/e-commerce/scroll/computers/tablets
```

It scrolls the page multiple times to simulate infinite scroll behavior and extracts product cards.

## ğŸ§¹ Optional: Clean Up

To stop and remove containers:

```bash
docker-compose down
```

To also remove the persistent volume (will delete all stored data):

```bash
docker-compose down -v
```

## ğŸ“„ License

MIT License â€“ see [LICENSE](LICENSE) for details.